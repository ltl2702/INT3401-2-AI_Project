{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11449400,"sourceType":"datasetVersion","datasetId":6988906}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/karinae/trainmodel-pnn-scg?scriptVersionId=237693068\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter('ignore', FutureWarning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:45:11.809791Z","iopub.execute_input":"2025-05-03T07:45:11.809996Z","iopub.status.idle":"2025-05-03T07:45:15.16959Z","shell.execute_reply.started":"2025-05-03T07:45:11.809979Z","shell.execute_reply":"2025-05-03T07:45:15.168778Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install scikit-learn==1.2.2 imbalanced-learn==0.10.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:45:15.170404Z","iopub.execute_input":"2025-05-03T07:45:15.17078Z","iopub.status.idle":"2025-05-03T07:45:19.725165Z","shell.execute_reply.started":"2025-05-03T07:45:15.170757Z","shell.execute_reply":"2025-05-03T07:45:19.72443Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data đã xử lý","metadata":{}},{"cell_type":"code","source":"df_april = pd.read_csv('/kaggle/input/clean-aws-month/final_data/filled_data_april.csv')\ndf_october = pd.read_csv('/kaggle/input/clean-aws-month/final_data/filled_data_october.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:45:19.726163Z","iopub.execute_input":"2025-05-03T07:45:19.726416Z","iopub.status.idle":"2025-05-03T07:45:27.955414Z","shell.execute_reply.started":"2025-05-03T07:45:19.726394Z","shell.execute_reply":"2025-05-03T07:45:27.95481Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_april.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:45:27.95613Z","iopub.execute_input":"2025-05-03T07:45:27.956324Z","iopub.status.idle":"2025-05-03T07:45:27.990885Z","shell.execute_reply.started":"2025-05-03T07:45:27.956307Z","shell.execute_reply":"2025-05-03T07:45:27.990198Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature đã chọn dựa trên phương án 1\n- T4:  CAPE, KX, R500, R850, TCLW, TCW, U250, U850, V250, V850, B10B, B11B, B14B, I2B, I4B, IRB, WVB\n- T10:  EWSS, KX, PEV, R250, R500, R850, SSHF, TCLW, TCW, U250, U850, V250, V850, B11B,  B14B, I4B, IRB","metadata":{}},{"cell_type":"code","source":"features_april = ['CAPE', 'KX', 'R500', 'R850', 'TCLW', 'TCW', 'U250', 'U850', 'V250', 'V850', 'B10B', 'B11B', 'B14B', 'I2B', 'I4B', 'IRB', 'WVB']\nfeatures_october = ['EWSS', 'KX', 'PEV', 'R250', 'R500', 'R850', 'SSHF', 'TCLW', 'TCW', 'U250', 'U850', 'V250', 'V850', 'B11B', 'B14B', 'I4B', 'IRB']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:45:27.991657Z","iopub.execute_input":"2025-05-03T07:45:27.991939Z","iopub.status.idle":"2025-05-03T07:45:27.996224Z","shell.execute_reply.started":"2025-05-03T07:45:27.991895Z","shell.execute_reply":"2025-05-03T07:45:27.995548Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Chia train test theo thời gian","metadata":{}},{"cell_type":"code","source":"def check_missing_hours(df, month):\n    df['datetime'] = pd.to_datetime(df['datetime']) \n    df = df[df['datetime'].dt.month == month].copy()\n\n    all_dates = df['datetime'].dt.date.unique()\n    all_expected_hours = []\n    for date in all_dates:\n        hours = pd.date_range(f\"{date} 00:00:00\", f\"{date} 23:00:00\", freq=\"H\")\n        all_expected_hours.extend(hours)\n    \n    expected_df = pd.DataFrame(all_expected_hours, columns=[\"datetime\"])\n    missing_times = expected_df[~expected_df['datetime'].isin(df['datetime'])]\n    return missing_times\n\nmissing_april = check_missing_hours(df_april, 4)\nmissing_october = check_missing_hours(df_october, 10)\n\nprint(\"T4:\")\nprint(missing_april['datetime'].dt.strftime('%Y-%m-%d %H:%M:%S').to_string(index=False))\n\nprint(\"T10:\")\nprint(missing_october['datetime'].dt.strftime('%Y-%m-%d %H:%M:%S').to_string(index=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:45:32.382191Z","iopub.execute_input":"2025-05-03T07:45:32.38286Z","iopub.status.idle":"2025-05-03T07:45:32.886601Z","shell.execute_reply.started":"2025-05-03T07:45:32.382827Z","shell.execute_reply":"2025-05-03T07:45:32.885779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_missing_hours_heatmap(missing_times, month_name):\n    missing_times['Year'] = missing_times['datetime'].dt.year\n    missing_times['Month'] = missing_times['datetime'].dt.month\n    missing_times['Day'] = missing_times['datetime'].dt.day\n    missing_times['Hour'] = missing_times['datetime'].dt.hour\n    \n    missing_counts = missing_times.groupby(['Year', 'Month', 'Day']).size().reset_index(name='Missing_Hours')\n\n    pivot_data = missing_counts.pivot_table(\n        index=['Year', 'Month'], columns='Day', values='Missing_Hours', fill_value=0\n    )\n    \n    pivot_data.index = [f'{year} - Tháng {month}' for year, month in pivot_data.index]\n\n    plt.figure(figsize=(16, 8))\n    sns.heatmap(pivot_data, cmap='YlGnBu', annot=True, fmt='g', linewidths=0.5, square=True, vmin=0, vmax=24)\n    plt.ylabel('Năm - Tháng', fontsize=12)\n    plt.show()\n\ncreate_missing_hours_heatmap(missing_october, \"Tháng 10\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:45:39.359184Z","iopub.execute_input":"2025-05-03T07:45:39.359866Z","iopub.status.idle":"2025-05-03T07:45:39.717334Z","shell.execute_reply.started":"2025-05-03T07:45:39.359838Z","shell.execute_reply":"2025-05-03T07:45:39.716544Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Tháng 4 thiếu giờ ở ngày 2020-04-06 và 2020-04-13\n- Tháng 10 thiếu giờ chủ yếu ở năm 2019 trong ngày 7,8,9,19,15-21,23,26,27,29,30\n\n  => quyết định chia tập train từ 4/4/2019 đến 28/4/2019 và 4/4/2020 đến 28/4/2020, tập test là những ngày còn lại\n -  tương tự cho tháng 10","metadata":{}},{"cell_type":"code","source":"def split_data_by_multiple_ranges(df, train_ranges):\n    train_mask = False\n    for start, end in train_ranges:\n        train_mask |= (df['datetime'] >= start) & (df['datetime'] < end)\n    train_df = df[train_mask]\n    test_df = df[~train_mask]\n    return train_df, test_df\n\ndef convert_rain_label(df):\n    df['AWS'] = df['AWS'].apply(lambda x: 1 if x > 0 else 0)\n    return df\n\ndf_april = convert_rain_label(df_april)\ndf_october = convert_rain_label(df_october)\n\ntrain_ranges_april = [(\"2019-04-04\", \"2019-04-29\"), (\"2020-04-04\", \"2020-04-29\")]\ntrain_ranges_october = [(\"2019-10-04\", \"2019-10-29\"), (\"2020-10-04\", \"2020-10-29\")]\n\ntrain_april, test_april = split_data_by_multiple_ranges(df_april, train_ranges_april)\ntrain_october, test_october = split_data_by_multiple_ranges(df_october, train_ranges_october)\n\nprint(f\"Tháng 4 - Train: {train_april.shape}\")\nprint(f\"Tháng 4 - Test: {test_april.shape}\")\nprint(f\"Tháng 10 - Train: {train_october.shape}\")\nprint(f\"Tháng 10 - Test: {test_october.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:45:43.219654Z","iopub.execute_input":"2025-05-03T07:45:43.219941Z","iopub.status.idle":"2025-05-03T07:45:43.539008Z","shell.execute_reply.started":"2025-05-03T07:45:43.219895Z","shell.execute_reply":"2025-05-03T07:45:43.538264Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rain_april = train_april[train_april['AWS'] == 1].shape[0]\nno_rain_april = train_april[train_april['AWS'] == 0].shape[0]\ntotal_april = train_april.shape[0]\nrain_ratio_april = rain_april / total_april\nno_rain_ratio_april = no_rain_april / total_april\n\nrain_october = train_october[train_october['AWS'] == 1].shape[0]\nno_rain_october = train_october[train_october['AWS'] == 0].shape[0]\ntotal_october = train_october.shape[0]\nrain_ratio_october = rain_october / total_october\nno_rain_ratio_october = no_rain_october / total_october\n\nlabels = ['Mưa', 'Không mưa']\nrain_data_april = [rain_ratio_april, no_rain_ratio_april]\nrain_data_october = [rain_ratio_october, no_rain_ratio_october]\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\naxes[0].pie(rain_data_april, labels=labels, autopct='%1.2f%%', startangle=90, colors=['#1f77b4', '#ff7f0e'])\naxes[0].set_title(f\"Tỷ lệ mưa và không mưa - Tháng 4\")\n\naxes[1].pie(rain_data_october, labels=labels, autopct='%1.2f%%', startangle=90, colors=['#1f77b4', '#ff7f0e'])\naxes[1].set_title(f\"Tỷ lệ mưa và không mưa - Tháng 10\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:45:48.169176Z","iopub.execute_input":"2025-05-03T07:45:48.169445Z","iopub.status.idle":"2025-05-03T07:45:48.435219Z","shell.execute_reply.started":"2025-05-03T07:45:48.169427Z","shell.execute_reply":"2025-05-03T07:45:48.434522Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Min-max normalization","metadata":{}},{"cell_type":"code","source":"train_april.shape[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:45:58.005852Z","iopub.execute_input":"2025-05-03T07:45:58.006602Z","iopub.status.idle":"2025-05-03T07:45:58.01095Z","shell.execute_reply.started":"2025-05-03T07:45:58.006581Z","shell.execute_reply":"2025-05-03T07:45:58.010399Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_april = train_april[features_april]\ny_train_april = train_april['AWS']\n\nX_test_april = test_april[features_april]\ny_test_april = test_april['AWS']\n\nX_train_october = train_october[features_october]\ny_train_october = train_october['AWS']\n\nX_test_october = test_october[features_october]\ny_test_october = test_october['AWS']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:45:58.265049Z","iopub.execute_input":"2025-05-03T07:45:58.265327Z","iopub.status.idle":"2025-05-03T07:45:58.305802Z","shell.execute_reply.started":"2025-05-03T07:45:58.265309Z","shell.execute_reply":"2025-05-03T07:45:58.305247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler_april = MinMaxScaler()\nX_train_april_scaled = scaler_april.fit_transform(X_train_april)\nX_test_april_scaled = scaler_april.transform(X_test_april)\n\nscaler_october = MinMaxScaler()\nX_train_october_scaled = scaler_october.fit_transform(X_train_october)\nX_test_october_scaled = scaler_october.transform(X_test_october)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:46:02.156092Z","iopub.execute_input":"2025-05-03T07:46:02.156364Z","iopub.status.idle":"2025-05-03T07:46:02.348733Z","shell.execute_reply.started":"2025-05-03T07:46:02.156346Z","shell.execute_reply":"2025-05-03T07:46:02.348118Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_april_scaled = pd.DataFrame(X_train_april_scaled, columns=X_train_april.columns)\nX_test_april_scaled = pd.DataFrame(X_test_april_scaled, columns=X_test_april.columns)\n\nX_train_october_scaled = pd.DataFrame(X_train_october_scaled, columns=X_train_october.columns)\nX_test_october_scaled = pd.DataFrame(X_test_october_scaled, columns=X_test_october.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:46:05.370646Z","iopub.execute_input":"2025-05-03T07:46:05.371321Z","iopub.status.idle":"2025-05-03T07:46:05.37554Z","shell.execute_reply.started":"2025-05-03T07:46:05.371297Z","shell.execute_reply":"2025-05-03T07:46:05.374899Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train lúc chưa cân bằng","metadata":{}},{"cell_type":"code","source":"def train_and_evaluate_multiple_models(models_info, model_class, **model_params):\n    trained_models = {}\n    predictions = {}\n    \n    for name, X_train, y_train, X_test, y_test in models_info:\n        print(f\"\\n{name}...\")\n        \n        # Khởi tạo mô hình với các tham số đầu vào\n        model = model_class(**model_params)\n        model.fit(X_train, y_train)\n        trained_models[name] = model\n        \n        # Dự đoán trên tập test\n        predictions[name] = model.predict(X_test)\n\n    # Đánh giá mô hình\n    for name, X_train, y_train, X_test, y_test in models_info:\n        print(f\"\\n{name}:\")\n        y_pred = predictions[name]\n        \n        # In classification report\n        print(classification_report(y_test, y_pred, target_names=['No Rain', 'Rain']))\n        \n        # Hiển thị confusion matrix\n        cm = confusion_matrix(y_test, y_pred)\n        cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No Rain', 'Rain'])\n        cm_display.plot(cmap=plt.cm.Blues)\n        plt.title(f\"Confusion Matrix - {name}\")\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:46:06.058452Z","iopub.execute_input":"2025-05-03T07:46:06.05876Z","iopub.status.idle":"2025-05-03T07:46:06.064285Z","shell.execute_reply.started":"2025-05-03T07:46:06.058742Z","shell.execute_reply":"2025-05-03T07:46:06.063619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"models_info = [\n    (\"Tháng 4\", X_train_april_scaled, y_train_april, X_test_april_scaled, y_test_april),\n    (\"Tháng 10\", X_train_october_scaled, y_train_october, X_test_october_scaled, y_test_october)\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:46:09.013832Z","iopub.execute_input":"2025-05-03T07:46:09.014554Z","iopub.status.idle":"2025-05-03T07:46:09.017977Z","shell.execute_reply.started":"2025-05-03T07:46:09.014531Z","shell.execute_reply":"2025-05-03T07:46:09.017239Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:33:01.733762Z","iopub.execute_input":"2025-05-02T15:33:01.734056Z","iopub.status.idle":"2025-05-02T15:33:02.104518Z","shell.execute_reply.started":"2025-05-02T15:33:01.734037Z","shell.execute_reply":"2025-05-02T15:33:02.10398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_params = {\n    'n_estimators': 100,\n    'random_state': 42\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:33:02.105574Z","iopub.execute_input":"2025-05-02T15:33:02.105988Z","iopub.status.idle":"2025-05-02T15:33:02.109283Z","shell.execute_reply.started":"2025-05-02T15:33:02.105964Z","shell.execute_reply":"2025-05-02T15:33:02.108678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_and_evaluate_multiple_models(models_info, RandomForestClassifier, **model_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:33:02.41587Z","iopub.execute_input":"2025-05-02T15:33:02.41633Z","iopub.status.idle":"2025-05-02T15:37:27.927651Z","shell.execute_reply.started":"2025-05-02T15:33:02.416312Z","shell.execute_reply":"2025-05-02T15:37:27.926992Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Tháng 4:\n              precision    recall  f1-score   support\n\n     No Rain       0.90      0.99      0.95     46790\n        Rain       0.52      0.06      0.11      5289\n\n    accuracy                           0.90     52079\n    macro avg       0.71      0.53      0.53     52079\n    weighted avg       0.86      0.90      0.86     52079\n\n\nTháng 10:\n              precision    recall  f1-score   support\n\n     No Rain       0.89      0.96      0.92     57109\n        Rain       0.73      0.50      0.59     13583\n\n    accuracy                           0.87     70692\n    macro avg       0.81      0.73      0.76     70692\n    weighted avg       0.86      0.87      0.86     70692\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## XGboost","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:37:27.929005Z","iopub.execute_input":"2025-05-02T15:37:27.929443Z","iopub.status.idle":"2025-05-02T15:37:28.165437Z","shell.execute_reply.started":"2025-05-02T15:37:27.929426Z","shell.execute_reply":"2025-05-02T15:37:28.164912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgboost_params = {\n    'n_estimators': 100,\n    'random_state': 42\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:37:28.166118Z","iopub.execute_input":"2025-05-02T15:37:28.166352Z","iopub.status.idle":"2025-05-02T15:37:28.16986Z","shell.execute_reply.started":"2025-05-02T15:37:28.166326Z","shell.execute_reply":"2025-05-02T15:37:28.169137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_and_evaluate_multiple_models(models_info, XGBClassifier, **xgboost_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:37:28.17094Z","iopub.execute_input":"2025-05-02T15:37:28.17115Z","iopub.status.idle":"2025-05-02T15:37:31.71643Z","shell.execute_reply.started":"2025-05-02T15:37:28.171135Z","shell.execute_reply":"2025-05-02T15:37:31.71569Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Tháng 4:\n              precision    recall  f1-score   support\n\n     No Rain       0.91      0.98      0.94     46790\n        Rain       0.39      0.10      0.16      5289\n\n    accuracy                           0.89     52079\n    macro avg       0.65      0.54      0.55     52079\n    weighted avg       0.85      0.89      0.86     52079\n\n\nTháng 10:\n              precision    recall  f1-score   support\n\n     No Rain       0.90      0.94      0.92     57109\n        Rain       0.71      0.58      0.64     13583\n\n    accuracy                           0.87     70692\n    macro avg       0.81      0.76      0.78     70692\n    weighted avg       0.87      0.87      0.87     70692","metadata":{}},{"cell_type":"markdown","source":"## SVM","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T08:35:58.151507Z","iopub.execute_input":"2025-05-02T08:35:58.152161Z","iopub.status.idle":"2025-05-02T08:35:58.15538Z","shell.execute_reply.started":"2025-05-02T08:35:58.152139Z","shell.execute_reply":"2025-05-02T08:35:58.154531Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"svm_params = {\n    'kernel': 'rbf',       \n    'C': 1.0,\n    'gamma': 'scale',       \n    'probability': True,     \n    'random_state': 42\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T08:35:58.408973Z","iopub.execute_input":"2025-05-02T08:35:58.409418Z","iopub.status.idle":"2025-05-02T08:35:58.41293Z","shell.execute_reply.started":"2025-05-02T08:35:58.409398Z","shell.execute_reply":"2025-05-02T08:35:58.412237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_and_evaluate_multiple_models(models_info, SVC, **svm_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T08:36:05.537488Z","iopub.execute_input":"2025-05-02T08:36:05.538171Z","iopub.status.idle":"2025-05-02T08:36:05.561634Z","shell.execute_reply.started":"2025-05-02T08:36:05.538146Z","shell.execute_reply":"2025-05-02T08:36:05.56084Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PNN","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics.pairwise import pairwise_distances\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:46:27.182114Z","iopub.execute_input":"2025-05-03T07:46:27.182658Z","iopub.status.idle":"2025-05-03T07:46:27.26304Z","shell.execute_reply.started":"2025-05-03T07:46:27.182635Z","shell.execute_reply":"2025-05-03T07:46:27.262514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import pairwise_distances\n\nclass RBFLayer:\n    def __init__(self, centers, sigma=1.0):\n        self.centers = centers\n        self.sigma = sigma\n\n    def compute(self, X):\n        diff = X[:, np.newaxis, :] - self.centers\n        dist_sq = np.sum(diff**2, axis=2)\n        return np.exp(-dist_sq / (2 * self.sigma**2))\n\ndef softmax(z):\n    # z: (n_samples, n_classes)\n    z_max = np.max(z, axis=1, keepdims=True)\n    e_z = np.exp(z - z_max)\n    return e_z / e_z.sum(axis=1, keepdims=True)\n\n\n#Define PNN Model\nclass PNN:\n    def __init__(self, sigma=1.0, batch_size=1000):\n        self.sigma = sigma\n        self.centers = None\n        self.batch_size = batch_size\n\n    def fit(self, X_train, y_train):\n        if isinstance(X_train, pd.DataFrame):\n            X_train = X_train.values\n        else:\n            X_train = X_train\n        if isinstance(y_train, pd.Series):\n            y_train = y_train.values\n        else:\n            y_train = y_train\n\n        classes = np.unique(y_train)\n        centers = []\n        for c in classes:\n            centers.append(X_train[y_train == c].mean(axis=0))\n        self.centers = np.vstack(centers)\n        self.rbf = RBFLayer(self.centers, sigma=self.sigma)\n\n    def predict_proba(self, X):\n        if isinstance(X, pd.DataFrame):\n            X = X.values\n        else:\n            X = X\n            \n        n = X.shape[0]\n        if self.batch_size is None:\n            raw = self.rbf.compute(X)\n            return softmax(raw)\n        # batch processing\n        probs = []\n        for start in range(0, n, self.batch_size):\n            end = min(start + self.batch_size, n)\n            raw = self.rbf.compute(X[start:end])\n            probs.append(softmax(raw))\n        return np.vstack(probs)\n\n    def predict(self, X):\n        # trả về nhãn 0 hoặc 1\n        probs = self.predict_proba(X)\n        return np.argmax(probs, axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:46:27.443165Z","iopub.execute_input":"2025-05-03T07:46:27.443444Z","iopub.status.idle":"2025-05-03T07:46:27.453874Z","shell.execute_reply.started":"2025-05-03T07:46:27.443423Z","shell.execute_reply":"2025-05-03T07:46:27.453259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Fine-tune + train\nsigmas = np.logspace(-2, 1, 50)\nparam_grid = {'sigma': sigmas.tolist()}  # Grid search for sigma values\n\nbest_score = 0\nbest_sigma = 0\n\n# Perform grid search over the values of sigma\nprint(\"Fine tuning...\")\nfor name, train_X, train_Y, X_test, y_test in models_info:\n    X_train, X_val, y_train, y_val = train_test_split(\n        train_X, train_Y, test_size=0.2, random_state=42\n    )\n    print(f\"{name}...\")\n    for sigma in param_grid['sigma']:\n        pnn_batch = PNN(sigma=sigma, batch_size=2048)\n        pnn_batch.fit(X_train, y_train)\n        y_pred = pnn_batch.predict(X_val)\n        score = classification_report(y_val, y_pred, zero_division=0, output_dict=True)['macro avg']['f1-score']\n    \n        if score > best_score:\n            best_score = score\n            best_sigma = sigma\n\n    print(f\"Best sigma: {best_sigma} with f1: {best_score}\")\n\n# Final evaluation with the best sigma\nprint(\"Train with best param...\")\nfor name, X_train, y_train, X_test, y_test in models_info:\n    print(f\"{name}...\")\n    print(X_train.shape)\n    pnn_batch = PNN(sigma=best_sigma, batch_size=1000)\n    pnn_batch.fit(X_train, y_train)\n    y_pred = pnn_batch.predict(X_test)\n\n    print(\"Classification Report:\")\n    print(classification_report(y_test, y_pred))\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(y_test, y_pred))\n\n    # Optionally, plot the confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Rain\", \"Rain\"], yticklabels=[\"No Rain\", \"Rain\"])\n    plt.title(\"Confusion Matrix\")\n    plt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T16:03:57.89363Z","iopub.execute_input":"2025-05-02T16:03:57.894381Z","iopub.status.idle":"2025-05-02T16:04:09.612138Z","shell.execute_reply.started":"2025-05-02T16:03:57.894357Z","shell.execute_reply":"2025-05-02T16:04:09.611397Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SCG","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:46:34.335378Z","iopub.execute_input":"2025-05-03T07:46:34.336048Z","iopub.status.idle":"2025-05-03T07:46:34.340756Z","shell.execute_reply.started":"2025-05-03T07:46:34.336017Z","shell.execute_reply":"2025-05-03T07:46:34.340061Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def softmax(Z):\n    # Ensure Z is a numpy array of shape (n_samples, n_classes)\n    Z_arr = Z.values if hasattr(Z, \"values\") else np.array(Z)\n    # 1) subtract the max for numerical stability\n    #    max_per_row: shape (n_samples,)\n    max_per_row = np.max(Z_arr, axis=1)\n    #    reshape to (n_samples, 1) so broadcasting works\n    max_per_row = max_per_row.reshape(-1, 1)\n    # 2) exponentiate\n    expZ = np.exp(Z_arr - max_per_row)\n    # 3) sum per row, then reshape to (n_samples, 1)\n    sum_per_row = np.sum(expZ, axis=1).reshape(-1, 1)\n    # 4) divide\n    return expZ / sum_per_row\n\ndef pack(W1, b1, W2, b2):\n    \"\"\"\n    Flatten and concatenate parameters, handling pandas DataFrame/Series inputs.\n    \"\"\"\n    # Convert DataFrame/Series to numpy arrays\n    W1_arr = W1.values if isinstance(W1, pd.DataFrame) else np.array(W1)\n    b1_arr = b1.values if isinstance(b1, (pd.Series, pd.DataFrame)) else np.array(b1)\n    W2_arr = W2.values if isinstance(W2, pd.DataFrame) else np.array(W2)\n    b2_arr = b2.values if isinstance(b2, (pd.Series, pd.DataFrame)) else np.array(b2)\n    # Flatten and concatenate\n    return np.hstack([\n        W1_arr.ravel(),\n        b1_arr.ravel(),\n        W2_arr.ravel(),\n        b2_arr.ravel()\n    ])\n\ndef unpack(p, input_dim, hidden_dim, output_dim):\n    idx = 0\n    W1 = p[idx:idx + hidden_dim*input_dim].reshape(hidden_dim, input_dim)\n    idx += hidden_dim*input_dim\n    b1 = p[idx:idx + hidden_dim]\n    idx += hidden_dim\n    W2 = p[idx:idx + output_dim*hidden_dim].reshape(output_dim, hidden_dim)\n    idx += output_dim*hidden_dim\n    b2 = p[idx:idx + output_dim]\n    return W1, b1, W2, b2\n\ndef loss_and_grad(p, X_train, Y_train_oh, input_dim, hidden_dim, output_dim):\n    W1, b1, W2, b2 = unpack(p, input_dim, hidden_dim, output_dim)\n    m = X_train.shape[0]\n    Z1 = X_train.dot(W1.T) + b1\n    A1 = np.tanh(Z1)\n    Z2 = A1.dot(W2.T) + b2\n    A2 = softmax(Z2)\n    loss = -np.mean(np.sum(Y_train_oh * np.log(A2 + 1e-12), axis=1))\n    dZ2 = (A2 - Y_train_oh) / m\n    dW2 = dZ2.T.dot(A1)\n    db2 = dZ2.sum(axis=0)\n    dA1 = dZ2.dot(W2)\n    dZ1 = dA1 * (1 - np.tanh(Z1)**2)\n    dW1 = dZ1.T.dot(X_train)\n    db1 = dZ1.sum(axis=0)\n    return loss, pack(dW1, db1, dW2, db2)\n\ndef trainscg(loss_grad_func, x0, epochs=8, **lg_kwargs):\n    sigma0, lambd = 1e-6, 1e-6\n    x = x0.copy()\n    f, g = loss_grad_func(x, **lg_kwargs)\n    d = -g\n    errors = [f]\n    for epoch in range(epochs):\n        mu = d.dot(d)\n        sigma = sigma0 / np.sqrt(mu) if mu > 0 else sigma0\n        _, g1 = loss_grad_func(x + sigma*d, **lg_kwargs)\n        s = (g1 - g) / sigma\n        delta = d.dot(s)\n        if delta <= 0:\n            delta = lambd * mu\n            lambd -= delta / mu\n        alpha = -d.dot(g) / (delta + lambd*mu)\n        x_new = x + alpha*d\n        f_new, g_new = loss_grad_func(x_new, **lg_kwargs)\n        Delta = 2*(f - f_new) / (alpha * d.dot(g))\n        if Delta >= 0:\n            x, f, g = x_new, f_new, g_new\n            lambd *= max(1/3, 1 - (2*Delta - 1)**3)\n            beta = (g.dot(g) - g.dot(g_new)) / (d.dot(g))\n            d = -g + beta*d\n        else:\n            lambd += mu * (1 - Delta)\n        errors.append(f)\n        print(f\"Epoch {epoch+1}/{epochs} — loss: {f:.6f}\")\n    return x, errors\n\n# ——— Main loop per dataset ———\nfor name, X_train, y_train, X_test, y_test in models_info:\n    print(f\"\\n=== Dataset: {name} ===\")\n    y_arr = y_train.to_numpy().reshape(-1, 1)\n    encoder = OneHotEncoder(sparse=False, categories='auto')\n    Y_train_oh = encoder.fit_transform(y_arr)\n    \n    input_dim  = X_train.shape[1]\n    hidden_dim = 8\n    output_dim = Y_train_oh.shape[1]  # should be 2\n    \n    # 3) Initialize parameters\n    param_size  = hidden_dim*input_dim + hidden_dim + output_dim*hidden_dim + output_dim\n    init_params = 0.01 * np.random.randn(param_size)\n    \n    # 4) Train with SCG\n    opt_params, loss_history = trainscg(\n        loss_and_grad,\n        init_params,\n        epochs=8,\n        X_train=X_train,\n        Y_train_oh=Y_train_oh,\n        input_dim=input_dim,\n        hidden_dim=hidden_dim,\n        output_dim=output_dim\n    )\n    \n    # 5) Unpack weights & biases\n    W1_opt, b1_opt, W2_opt, b2_opt = unpack(opt_params, input_dim, hidden_dim, output_dim)\n    \n    # 6) Display performance curve\n    plt.figure()\n    plt.plot(range(len(loss_history)), loss_history, marker='o')\n    plt.title(f\"{name} – SCG loss curve\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Cross-entropy loss\")\n    plt.grid(True)\n    plt.show()\n    \n    # 7) ROC & final eval\n    def predict_proba(params, X):\n        W1, b1, W2, b2 = unpack(params, input_dim, hidden_dim, output_dim)\n        A1 = np.tanh(X.dot(W1.T) + b1)\n        Z2 = A1.dot(W2.T) + b2\n        return softmax(Z2)\n    \n    probs = predict_proba(opt_params, X_test)\n    y_pred = np.argmax(probs, axis=1)\n    \n    # ROC\n    fpr, tpr, _ = roc_curve(y_test, probs[:,1])\n    roc_auc = auc(fpr, tpr)\n    plt.figure()\n    plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n    plt.plot([0,1],[0,1],'k--')\n    plt.title(f\"{name} – ROC curve\")\n    plt.xlabel(\"False positive rate\")\n    plt.ylabel(\"True positive rate\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n    \n    # Classification report + confusion matrix\n    print(\"Classification Report:\")\n    print(classification_report(y_test, y_pred))\n    cm = confusion_matrix(y_test, y_pred)\n    print(\"Confusion Matrix:\\n\", cm)\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n                xticklabels=encoder.categories_[0], yticklabels=encoder.categories_[0])\n    plt.title(f\"{name} – Confusion matrix\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:57:01.712274Z","iopub.execute_input":"2025-05-03T07:57:01.712509Z","iopub.status.idle":"2025-05-03T07:57:07.857852Z","shell.execute_reply.started":"2025-05-03T07:57:01.712492Z","shell.execute_reply":"2025-05-03T07:57:07.857233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import f1_score, classification_report, confusion_matrix, roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ——— Utility functions ———\ndef softmax(Z):\n    # Ensure Z is a numpy array of shape (n_samples, n_classes)\n    Z_arr = Z.values if hasattr(Z, \"values\") else np.array(Z)\n    # 1) subtract the max for numerical stability\n    #    max_per_row: shape (n_samples,)\n    max_per_row = np.max(Z_arr, axis=1)\n    #    reshape to (n_samples, 1) so broadcasting works\n    max_per_row = max_per_row.reshape(-1, 1)\n    # 2) exponentiate\n    expZ = np.exp(Z_arr - max_per_row)\n    # 3) sum per row, then reshape to (n_samples, 1)\n    sum_per_row = np.sum(expZ, axis=1).reshape(-1, 1)\n    # 4) divide\n    return expZ / sum_per_row\n\ndef pack(W1, b1, W2, b2):\n    \"\"\"\n    Flatten and concatenate parameters, handling pandas DataFrame/Series inputs.\n    \"\"\"\n    # Convert DataFrame/Series to numpy arrays\n    W1_arr = W1.values if isinstance(W1, pd.DataFrame) else np.array(W1)\n    b1_arr = b1.values if isinstance(b1, (pd.Series, pd.DataFrame)) else np.array(b1)\n    W2_arr = W2.values if isinstance(W2, pd.DataFrame) else np.array(W2)\n    b2_arr = b2.values if isinstance(b2, (pd.Series, pd.DataFrame)) else np.array(b2)\n    # Flatten and concatenate\n    return np.hstack([\n        W1_arr.ravel(),\n        b1_arr.ravel(),\n        W2_arr.ravel(),\n        b2_arr.ravel()\n    ])\n\ndef unpack(p, input_dim, hidden_dim, output_dim):\n    idx = 0\n    W1 = p[idx:idx + hidden_dim*input_dim].reshape(hidden_dim, input_dim)\n    idx += hidden_dim*input_dim\n    b1 = p[idx:idx + hidden_dim]\n    idx += hidden_dim\n    W2 = p[idx:idx + output_dim*hidden_dim].reshape(output_dim, hidden_dim)\n    idx += output_dim*hidden_dim\n    b2 = p[idx:idx + output_dim]\n    return W1, b1, W2, b2\n\ndef trainscg(loss_grad_func, x0, epochs=8, **lg_kwargs):\n    sigma0, lambd = 1e-6, 1e-6\n    x = x0.copy()\n    f, g = loss_grad_func(x, **lg_kwargs)\n    d = -g\n    errors = [f]\n    for epoch in range(epochs):\n        mu = d.dot(d)\n        sigma = sigma0 / np.sqrt(mu) if mu > 0 else sigma0\n        _, g1 = loss_grad_func(x + sigma*d, **lg_kwargs)\n        s = (g1 - g) / sigma\n        delta = d.dot(s)\n        if delta <= 0:\n            delta = lambd * mu\n            lambd -= delta / mu\n        alpha = -d.dot(g) / (delta + lambd*mu)\n        x_new = x + alpha*d\n        f_new, g_new = loss_grad_func(x_new, **lg_kwargs)\n        Delta = 2*(f - f_new) / (alpha * d.dot(g))\n        if Delta >= 0:\n            x, f, g = x_new, f_new, g_new\n            lambd *= max(1/3, 1 - (2*Delta - 1)**3)\n            beta = (g.dot(g) - g.dot(g_new)) / (d.dot(g))\n            d = -g + beta*d\n        else:\n            lambd += mu * (1 - Delta)\n        errors.append(f)\n    return x, errors\n\ndef loss_grad_factory(X, Y_oh, input_dim, hidden_dim, output_dim):\n    def loss_and_grad(p):\n        W1, b1, W2, b2 = unpack(p, input_dim, hidden_dim, output_dim)\n        m = X.shape[0]\n        Z1 = X.dot(W1.T) + b1\n        A1 = np.tanh(Z1)\n        Z2 = A1.dot(W2.T) + b2\n        A2 = softmax(Z2)\n        loss = -np.mean(np.sum(Y_oh * np.log(A2 + 1e-12), axis=1))\n        dZ2 = (A2 - Y_oh) / m\n        dW2 = dZ2.T.dot(A1)\n        db2 = dZ2.sum(axis=0)\n        dA1 = dZ2.dot(W2)\n        dZ1 = dA1 * (1 - np.tanh(Z1)**2)\n        dW1 = dZ1.T.dot(X)\n        db1 = dZ1.sum(axis=0)\n        return loss, pack(dW1, db1, dW2, db2)\n    return loss_and_grad\n\n# ——— Main grid search & final train ———\nparam_grid = {'hidden_dim': [10, 20, 50, 100, 150, 17, 25, 34, 50, 68, 85]}\n\nfor name, X_trval, y_trval, X_test, y_test in models_info:\n    print(f\"\\n=== Dataset: {name} ===\")\n    # split train+val from test\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_trval, y_trval, test_size=0.2, random_state=42\n    )\n    # one-hot encode y_train\n    y_arr = y_train.to_numpy().reshape(-1, 1)\n    encoder = OneHotEncoder(sparse=False, categories='auto')\n    Y_train_oh = encoder.fit_transform(y_arr.reshape(-1,1))\n    y_arv = y_val.to_numpy().reshape(-1, 1)\n    Y_val_oh   = encoder.transform(y_arv.reshape(-1,1))\n    # final test encoding not needed for training\n    \n    # scale assumed done before, X_trval and X_test are already scaled\n    \n    input_dim  = X_train.shape[1]\n    output_dim = Y_train_oh.shape[1]\n    \n    best_h, best_loss = None, np.inf\n    for h in param_grid['hidden_dim']:\n        # init params\n        param_size  = h*input_dim + h + output_dim*h + output_dim\n        init_p = 0.01 * np.random.randn(param_size)\n        # train SCG\n        lg = loss_grad_factory(X_train, Y_train_oh, input_dim, h, output_dim)\n        opt_p, _ = trainscg(lg, init_p, epochs=8)\n        # predict on val\n        W1, b1, W2, b2 = unpack(opt_p, input_dim, h, output_dim)\n        Z1_val = X_val.dot(W1.T) + b1\n        A1_val = np.tanh(Z1_val)\n        Z2_val = A1_val.dot(W2.T) + b2\n        A2_val = softmax(Z2_val)\n        val_loss = -np.mean(np.sum(Y_val_oh * np.log(A2_val + 1e-12), axis=1))\n        print(f\" hidden_dim={h:<3} → val loss = {val_loss:.6f}\")\n        if val_loss < best_loss:\n            best_loss, best_h = val_loss, h\n    \n    print(f\"⇒ Best hidden_dim = {best_h}, validation loss = {best_loss:.6f}\")\n    \n    # retrain on full train+val\n    X_full = np.vstack([X_train, X_val])\n    y_full = np.hstack([y_train, y_val])\n    Y_full_oh = encoder.fit_transform(y_full.reshape(-1,1))\n    param_size  = best_h*input_dim + best_h + output_dim*best_h + output_dim\n    init_p_full = 0.01 * np.random.randn(param_size)\n    lg_full = loss_grad_factory(X_full, Y_full_oh, input_dim, best_h, output_dim)\n    opt_p_full, loss_hist = trainscg(lg_full, init_p_full, epochs=8)\n    \n    # evaluate on test\n    W1f, b1f, W2f, b2f = unpack(opt_p_full, input_dim, best_h, output_dim)\n    A1_test = np.tanh(X_test.dot(W1f.T) + b1f)\n    probs_test = softmax(A1_test.dot(W2f.T) + b2f)\n    preds_test = np.argmax(probs_test, axis=1)\n    \n    print(\"Test classification report:\")\n    print(classification_report(y_test, preds_test))\n    cm = confusion_matrix(y_test, preds_test)\n    print(\"Test confusion matrix:\\n\", cm)\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n    plt.title(f\"{name} – hidden_dim={best_h}\")\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:49:00.412092Z","iopub.execute_input":"2025-05-03T07:49:00.412379Z","iopub.status.idle":"2025-05-03T07:51:53.683748Z","shell.execute_reply.started":"2025-05-03T07:49:00.412362Z","shell.execute_reply":"2025-05-03T07:51:53.683136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import SGD\nfrom sklearn.preprocessing import LabelBinarizer\nimport numpy as np\n\ndef train_and_evaluate_scg_ann(models_info, hidden_neurons=8, epochs=8, learning_rate=0.01):\n    trained_models = {}\n    predictions = {}\n\n    for name, X_train, y_train, X_test, y_test in models_info:\n        print(f\"\\n{name} - SCG-ANN...\")\n\n        # One-hot encode labels\n        lb = LabelBinarizer()\n        y_train_bin = lb.fit_transform(y_train)\n        y_test_bin = lb.transform(y_test)\n\n        model = Sequential()\n        model.add(Dense(hidden_neurons, input_shape=(X_train.shape[1],), activation='tanh'))\n        model.add(Dense(2, activation='softmax'))\n\n        model.compile(optimizer=SGD(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n        model.fit(X_train, y_train_bin, epochs=epochs, verbose=0)\n\n        y_pred_prob = model.predict(X_test)\n        y_pred = np.argmax(y_pred_prob, axis=1)\n\n        trained_models[name] = model\n        predictions[name] = y_pred\n\n    for (name, _, _, _, y_test) in models_info:\n        y_pred = predictions[name]\n        print(f\"\\n{name} - SCG-ANN:\")\n        print(classification_report(y_test, y_pred, target_names=['No Rain', 'Rain']))\n\n        cm = confusion_matrix(y_test, y_pred)\n        cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No Rain', 'Rain'])\n        cm_display.plot(cmap=plt.cm.Blues)\n        plt.title(f\"Confusion Matrix - SCG-ANN - {name}\")\n        plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T16:10:23.962247Z","iopub.execute_input":"2025-05-02T16:10:23.962928Z","iopub.status.idle":"2025-05-02T16:10:35.726739Z","shell.execute_reply.started":"2025-05-02T16:10:23.962904Z","shell.execute_reply":"2025-05-02T16:10:35.726196Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cân bằng dữ liệu bằng 3 phương pháp SMOTE","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom imblearn.combine import SMOTEENN, SMOTETomek\nfrom imblearn.under_sampling import CondensedNearestNeighbour\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\ndef balance_datasets(models_info, method=\"smote\", random_state=42):\n    balanced_data = []\n\n    for name, X_train, y_train, X_test, y_test in models_info:\n        print(f\"\\n{name} - Đang cân bằng với phương pháp: {method.upper()}\")\n\n        if method == \"smote\":\n            sampler = SMOTE(random_state=random_state)\n        elif method == \"smoteenn\":\n            sampler = SMOTEENN(random_state=random_state)\n        elif method == \"smotecnn\":\n            # CNN chỉ hỗ trợ under-sampling, nên cần dùng SMOTE trước rồi mới dùng CNN\n            X_temp, y_temp = SMOTE(random_state=random_state).fit_resample(X_train, y_train)\n            sampler = CondensedNearestNeighbour()\n            X_res, y_res = sampler.fit_resample(X_temp, y_temp)\n\n            unique, counts = np.unique(y_res, return_counts=True)\n            print(f\"Số lượng mẫu sau cân bằng (SMOTECNN): {dict(zip(unique, counts))}\")\n            balanced_data.append((name, X_res, y_res, X_test, y_test))\n            continue\n        else:\n            raise ValueError(\"Phương pháp không hợp lệ. Chọn: smote, smoteenn hoặc smotecnn\")\n\n        X_res, y_res = sampler.fit_resample(X_train, y_train)\n        unique, counts = np.unique(y_res, return_counts=True)\n        print(f\"Số lượng mẫu sau cân bằng: {dict(zip(unique, counts))}\")\n\n        balanced_data.append((name, X_res, y_res, X_test, y_test))\n\n    return balanced_data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:46:50.064139Z","iopub.execute_input":"2025-05-03T07:46:50.064421Z","iopub.status.idle":"2025-05-03T07:46:50.59538Z","shell.execute_reply.started":"2025-05-03T07:46:50.064404Z","shell.execute_reply":"2025-05-03T07:46:50.594637Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SMOTE","metadata":{}},{"cell_type":"code","source":"balanced_models_smote = balance_datasets(models_info, method=\"smote\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:46:54.942596Z","iopub.execute_input":"2025-05-03T07:46:54.942872Z","iopub.status.idle":"2025-05-03T07:47:00.288423Z","shell.execute_reply.started":"2025-05-03T07:46:54.942855Z","shell.execute_reply":"2025-05-03T07:47:00.287527Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  Random Forest","metadata":{}},{"cell_type":"code","source":"train_and_evaluate_multiple_models(balanced_models_smote, RandomForestClassifier, **model_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:47:06.383087Z","iopub.execute_input":"2025-05-03T07:47:06.383349Z","iopub.status.idle":"2025-05-03T07:47:06.522901Z","shell.execute_reply.started":"2025-05-03T07:47:06.383333Z","shell.execute_reply":"2025-05-03T07:47:06.522053Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Tháng 4:\n              precision    recall  f1-score   support\n\n     No Rain       0.91      0.98      0.94     46790\n        Rain       0.41      0.12      0.19      5289\n\n    accuracy                           0.89     52079\n    macro avg       0.66      0.55      0.57     52079\n    weighted avg       0.86      0.89      0.87     52079\n\nTháng 10:\n              precision    recall  f1-score   support\n\n     No Rain       0.91      0.92      0.91     57109\n        Rain       0.64      0.62      0.63     13583\n\n    accuracy                           0.86     70692\n    macro avg       0.78      0.77      0.77     70692\n    weighted avg       0.86      0.86      0.86     70692","metadata":{}},{"cell_type":"code","source":"!pip install bayesian-optimization scikit-learn pandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:28:30.462537Z","iopub.execute_input":"2025-04-25T08:28:30.462786Z","iopub.status.idle":"2025-04-25T08:28:34.044653Z","shell.execute_reply.started":"2025-04-25T08:28:30.462761Z","shell.execute_reply":"2025-04-25T08:28:34.043737Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from bayes_opt import BayesianOptimization\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:28:34.045918Z","iopub.execute_input":"2025-04-25T08:28:34.046787Z","iopub.status.idle":"2025-04-25T08:28:34.074956Z","shell.execute_reply.started":"2025-04-25T08:28:34.046749Z","shell.execute_reply":"2025-04-25T08:28:34.074192Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_dict = {name: (X_res, y_res, X_test, y_test) for name, X_res, y_res, X_test, y_test in balanced_models_smote}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:28:34.075816Z","iopub.execute_input":"2025-04-25T08:28:34.076069Z","iopub.status.idle":"2025-04-25T08:28:34.080323Z","shell.execute_reply.started":"2025-04-25T08:28:34.076049Z","shell.execute_reply":"2025-04-25T08:28:34.079573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_objective(X_res, y_res):\n    def objective(n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features):\n        model = RandomForestClassifier(\n            n_estimators=int(n_estimators),\n            max_depth=int(max_depth),\n            min_samples_split=int(min_samples_split),\n            min_samples_leaf=int(min_samples_leaf),\n            max_features=min(max_features, 0.999),\n            class_weight='balanced',\n            random_state=42\n        )\n        score = cross_val_score(model, X_res, y_res, cv=3, scoring=\"f1\").mean()\n        return -score  # Maximize F1 → Minimize -F1\n    return objective","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:28:34.081183Z","iopub.execute_input":"2025-04-25T08:28:34.081522Z","iopub.status.idle":"2025-04-25T08:28:34.09749Z","shell.execute_reply.started":"2025-04-25T08:28:34.081495Z","shell.execute_reply":"2025-04-25T08:28:34.096715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pbounds = {\n    'n_estimators': (10, 250),\n    'max_depth': (1, 50),\n    'min_samples_split': (2, 25),\n    'min_samples_leaf': (1, 50),\n    'max_features': (0.1, 0.999)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:28:34.098312Z","iopub.execute_input":"2025-04-25T08:28:34.098576Z","iopub.status.idle":"2025-04-25T08:28:34.112789Z","shell.execute_reply.started":"2025-04-25T08:28:34.098555Z","shell.execute_reply":"2025-04-25T08:28:34.112017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# optimizers = {}\n# for month in [\"Tháng 4\", \"Tháng 10\"]:\n#     print(f\"\\n Đang tối ưu cho {month}...\")\n#     X_res, y_res, _, _ = data_dict[month]\n\n#     optimizer = BayesianOptimization(\n#         f=make_objective(X_res, y_res),\n#         pbounds=pbounds,\n#         random_state=42\n#     )\n#     optimizer.maximize(init_points=5, n_iter=15)\n#     optimizers[month] = optimizer\n#     print(f\" Best params for {month}: {optimizer.max}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:28:34.113779Z","iopub.execute_input":"2025-04-25T08:28:34.114129Z","iopub.status.idle":"2025-04-25T08:28:34.127292Z","shell.execute_reply.started":"2025-04-25T08:28:34.114103Z","shell.execute_reply":"2025-04-25T08:28:34.126503Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Best params for Tháng 4: {'target': -0.6630507661570126, 'params': {'max_depth': 1.9061486915036214, 'max_features': 0.23765197570597946, 'min_samples_leaf': 35.37131602207891, 'min_samples_split': 2.7201556302611016, 'n_estimators': 60.504709358608636}}\n\nBest params for Tháng 10: {'target': -0.793800280961404, 'params': {'max_depth': 2.00864022049432, 'max_features': 0.9719489570936329, 'min_samples_leaf': 41.78968939922066, 'min_samples_split': 6.883799545600351, 'n_estimators': 53.637992129704145}}\n","metadata":{}},{"cell_type":"code","source":"import joblib\n\nbest_params = {\n    \"Tháng 4\": {\n        'max_depth': int(round(1.9061)),\n        'max_features': 0.2376,\n        'min_samples_leaf': int(round(35.3713)),\n        'min_samples_split': int(round(2.7201)),\n        'n_estimators': int(round(60.5047)),\n    },\n    \"Tháng 10\": {\n        'max_depth': int(round(2.0086)),\n        'max_features': 0.9719,\n        'min_samples_leaf': int(round(41.7896)),\n        'min_samples_split': int(round(6.8838)),\n        'n_estimators': int(round(53.638)),\n    }\n}\n\nfor name in [\"Tháng 4\", \"Tháng 10\"]:\n    print(f\"\\n=== {name} ===\")\n    X_res, y_res, X_test, y_test = data_dict[name]\n   \n    params = best_params[name]\n    print(f\"Using parameters for {name}: {params}\")\n\n    model = RandomForestClassifier(**params, random_state=42)\n    model.fit(X_res, y_res)\n\n    model_filename = f\"model_{name.replace(' ', '_').lower()}_randomfs.pkl\"\n    joblib.dump(model, model_filename)\n\n    y_pred = model.predict(X_test)\n    print(confusion_matrix(y_test, y_pred))\n    print(classification_report(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:39:21.753767Z","iopub.execute_input":"2025-04-25T08:39:21.754124Z","iopub.status.idle":"2025-04-25T08:42:40.232843Z","shell.execute_reply.started":"2025-04-25T08:39:21.754099Z","shell.execute_reply":"2025-04-25T08:42:40.231863Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"=== Tháng 4 ===\n\n[[36740 10050]\n \n [ 2450  2839]]\n                    \n                     precision    recall  f1-score   support\n\n           0       0.94      0.79      0.85     46790\n           1       0.22      0.54      0.31      5289\n\n    accuracy                           0.76     52079\n    macro avg       0.58      0.66      0.58     52079\n    weighted avg       0.86      0.76      0.80     52079\n\n\n=== Tháng 10 ===\n\n[[48709  8400]\n \n [ 3725  9858]]\n             \n              precision    recall  f1-score   support\n\n           0       0.93      0.85      0.89     57109\n           1       0.54      0.73      0.62     13583\n\n    accuracy                           0.83     70692\n    macro avg       0.73      0.79      0.75     70692\n    weighted avg       0.85      0.83      0.84     70692","metadata":{}},{"cell_type":"markdown","source":"### thử điều chỉnh lại ngưỡng phân loại","metadata":{}},{"cell_type":"code","source":"# for name in [\"Tháng 4\", \"Tháng 10\"]:\n#     print(f\"\\n=== {name} ===\")\n#     X_res, y_res, X_test, y_test = data_dict[name]\n   \n#     params = best_params[name]\n#     print(f\"Using parameters for {name}: {params}\")\n\n#     model = RandomForestClassifier(**params, random_state=42)\n#     model.fit(X_res, y_res)\n\n#     model_filename = f\"model_{name.replace(' ', '_').lower()}_randomfs.pkl\"\n#     joblib.dump(model, model_filename)\n\n#     y_proba = model.predict_proba(X_test)[:, 1]  # Probability of class 1 (rain)\n\n#     # Adjust classification threshold (e.g., 0.4)\n#     threshold = 0.4\n#     y_pred_adjusted = (y_proba >= threshold).astype(int)  # Classify as rain (1) if probability >= threshold\n\n#     print(confusion_matrix(y_test, y_pred_adjusted))\n#     print(classification_report(y_test, y_pred_adjusted))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:46:53.866921Z","iopub.execute_input":"2025-04-25T08:46:53.867238Z","iopub.status.idle":"2025-04-25T08:50:09.609592Z","shell.execute_reply.started":"2025-04-25T08:46:53.867211Z","shell.execute_reply":"2025-04-25T08:50:09.60868Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"=== Tháng 4 ===\n\n [[25057 21733]\n\n [ 1389  3900]]\n \n              precision    recall  f1-score   support\n\n           0       0.95      0.54      0.68     46790\n           1       0.15      0.74      0.25      5289\n\n    accuracy                           0.56     52079\n    macro avg       0.55      0.64      0.47     52079\n    weighted avg       0.87      0.56      0.64     52079\n\n\n\n=== Tháng 10 ===\n\n[[39941 17168]\n\n [ 1934 11649]]\n  \n              precision    recall  f1-score   support\n\n           0       0.95      0.70      0.81     57109\n           1       0.40      0.86      0.55     13583\n\n    accuracy                           0.73     70692\n     macro avg       0.68      0.78      0.68     70692\n    weighted avg       0.85      0.73      0.76     70692","metadata":{}},{"cell_type":"markdown","source":"### XGBoost","metadata":{}},{"cell_type":"code","source":"# train_and_evaluate_multiple_models(balanced_models_smote, XGBClassifier, **xgboost_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:28:34.279632Z","iopub.status.idle":"2025-04-25T08:28:34.279903Z","shell.execute_reply.started":"2025-04-25T08:28:34.279775Z","shell.execute_reply":"2025-04-25T08:28:34.279788Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Tháng 4:\n              precision    recall  f1-score   support\n\n     No Rain       0.93      0.84      0.88     46790\n        Rain       0.23      0.41      0.29      5289\n\n    accuracy                           0.80     52079\n    macro avg       0.58      0.63      0.59     52079\n    weighted avg       0.86      0.80      0.82     52079\n\nTháng 10:\n              precision    recall  f1-score   support\n\n     No Rain       0.94      0.86      0.90     57109\n        Rain       0.56      0.76      0.64     13583\n\n    accuracy                           0.84     70692\n    macro avg       0.75      0.81      0.77     70692\n    weighted avg       0.86      0.84      0.85     70692","metadata":{}},{"cell_type":"markdown","source":"  - Trước khi áp dụng SMOTE, mô hình có độ recall thấp, dẫn đến khả năng phát hiện mưa kém, thường bỏ sót các trường hợp có mưa;  độ chính xác tổng thể cao, nhưng điều này có thể gây ra sự thiên vị với lớp \"Không mưa\" (do lớp này chiếm ưu thế), F1 score của lớp mưa thường thấp hoặc ở mức trung bình, cho thấy mô hình chưa thể nhận diện tốt lớp này.\n  - Sau khi áp dụng SMOTE, recall tăng lên, giúp mô hình phát hiện mưa tốt hơn và giảm thiểu việc bỏ sót, độ chính xác giảm nhẹ, vì mô hình phải đánh giá công bằng hơn giữa lớp \"Mưa\" và \"Không mưa\", nhưng đây là sự cải thiện về mặt cân bằng giữa hai lớp, F1 score của lớp mưa tăng hoặc giữ nguyên, đặc biệt là trong tháng 4, khi mà sự cải thiện trở nên rõ rệt, giúp mô hình đánh giá chính xác hơn đối với lớp mưa.","metadata":{}},{"cell_type":"markdown","source":"#### BayesianOptimization","metadata":{}},{"cell_type":"code","source":"def make_objective_xgb(X_res, y_res):\n    def objective(n_estimators, max_depth, learning_rate, subsample, colsample_bytree):\n        model = xgb.XGBClassifier(\n            n_estimators=int(n_estimators),\n            max_depth=int(max_depth),\n            learning_rate=learning_rate,\n            subsample=subsample,\n            colsample_bytree=colsample_bytree,\n            objective='binary:logistic',\n            random_state=42\n        )\n        score = cross_val_score(model, X_res, y_res, cv=3, scoring=\"f1\").mean()\n        return -score  # Tối đa hóa F1 → Minimize -F1\n    return objective\n\npbounds = {\n    'n_estimators': (50, 500),          \n    'max_depth': (3, 15),             \n    'learning_rate': (0.01, 0.3),        \n    'subsample': (0.5, 1.0),           \n    'colsample_bytree': (0.5, 1.0)  \n}\n\noptimizers = {}\n\n# for month in [\"Tháng 4\", \"Tháng 10\"]:\n#     print(f\"\\nĐang tối ưu hóa cho {month}...\")\n#     X_res, y_res, _, _ = data_dict[month] \n    \n#     optimizer = BayesianOptimization(\n#         f=make_objective_xgb(X_res, y_res),\n#         pbounds=pbounds,\n#         random_state=42\n#     )\n\n#     optimizer.maximize(init_points=5, n_iter=15)\n#     optimizers[month] = optimizer\n#     print(f\"Best params for {month}: {optimizer.max}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T09:05:02.866341Z","iopub.execute_input":"2025-04-25T09:05:02.866962Z","iopub.status.idle":"2025-04-25T09:28:10.273003Z","shell.execute_reply.started":"2025-04-25T09:05:02.866938Z","shell.execute_reply":"2025-04-25T09:28:10.272209Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Best params for Tháng 4: {'target': -0.7762891357630576, 'params': {'colsample_bytree': 0.9302889641232557, 'learning_rate': 0.03969592497150714, 'max_depth': 3.5260199963228143, 'n_estimators': 157.070149470272, 'subsample': 0.7736235928015441}}\n\nBest params for Tháng 10: {'target': -0.7895895690703282, 'params': {'colsample_bytree': 0.9319409105687462, 'learning_rate': 0.17591251629597351, 'max_depth': 7.995163045267633, 'n_estimators': 132.4638530051047, 'subsample': 0.6407280284500263}}\n\n","metadata":{}},{"cell_type":"code","source":"# best_params = {\n#     \"Tháng 4\": {\n#         'colsample_bytree': 0.9303,\n#         'learning_rate': 0.0397,\n#         'max_depth': int(round(3.526)),     \n#         'n_estimators': int(round(157.07)),\n#         'subsample': 0.7736\n#     },\n#     \"Tháng 10\": {\n#         'colsample_bytree': 0.9319,\n#         'learning_rate': 0.1759,\n#         'max_depth': int(round(7.995)),\n#         'n_estimators': int(round(132.46)),\n#         'subsample': 0.6407\n#     }\n# }\n\n# for name in [\"Tháng 4\", \"Tháng 10\"]:\n#     print(f\"\\n=== {name} ===\")\n#     X_res, y_res, X_test, y_test = data_dict[name]\n    \n#     params = best_params[name]\n#     print(f\"Using parameters for {name}: {params}\")\n\n#     model = XGBClassifier(\n#         **params,\n#         use_label_encoder=False,\n#         eval_metric='logloss',\n#         random_state=42\n#     )\n#     model.fit(X_res, y_res)\n\n#     model_filename = f\"model_{name.replace(' ', '_').lower()}_xgboost.pkl\"\n#     joblib.dump(model, model_filename)\n\n#     y_pred = model.predict(X_test)\n#     print(confusion_matrix(y_test, y_pred))\n#     print(classification_report(y_test, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T09:31:08.988944Z","iopub.execute_input":"2025-04-25T09:31:08.989266Z","iopub.status.idle":"2025-04-25T09:31:21.329202Z","shell.execute_reply.started":"2025-04-25T09:31:08.989246Z","shell.execute_reply":"2025-04-25T09:31:21.328361Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"=== Tháng 4 ===\n\n[[32146 14644]\n\n [ 1663  3626]]\n      \n              precision    recall  f1-score   support\n\n           0       0.95      0.69      0.80     46790\n           1       0.20      0.69      0.31      5289\n\n    accuracy                           0.69     52079\n    macro avg       0.57      0.69      0.55     52079\n    weighted avg       0.87      0.69      0.75     52079\n\n\n=== Tháng 10 ===\n\n[[50486  6623]\n\n[ 3885  9698]]\n             \n              precision    recall  f1-score   support\n\n           0       0.93      0.88      0.91     57109\n           1       0.59      0.71      0.65     13583\n\n    accuracy                           0.85     70692\n    macro avg       0.76      0.80      0.78     70692\n    weighted avg       0.86      0.85      0.86     70692","metadata":{}},{"cell_type":"markdown","source":"### PNN after SMOTE","metadata":{}},{"cell_type":"code","source":"#Fine-tune + train\nsigmas = np.logspace(-2, 1, 50)\nparam_grid = {'sigma': sigmas.tolist()}  # Grid search for sigma values\n\nbest_score = 0\nbest_sigma = 0\n\n# Perform grid search over the values of sigma\nprint(\"Fine tuning...\")\nfor name, train_X, train_Y, X_test, y_test in balanced_models_smote:\n    X_train, X_val, y_train, y_val = train_test_split(\n        train_X, train_Y, test_size=0.2, random_state=42\n    )\n    print(f\"{name}...\")\n    for sigma in param_grid['sigma']:\n        pnn_batch = PNN(sigma=sigma, batch_size=2048)\n        pnn_batch.fit(X_train, y_train)\n        y_pred = pnn_batch.predict(X_val)\n        score = classification_report(y_val, y_pred, zero_division=0, output_dict=True)['macro avg']['f1-score']\n    \n        if score > best_score:\n            best_score = score\n            best_sigma = sigma\n\n    print(f\"Best sigma: {best_sigma} with f1: {best_score}\")\n\n# Final evaluation with the best sigma\nprint(\"Train with best param...\")\nfor name, X_train, y_train, X_test, y_test in balanced_models_smote:\n    print(f\"{name}...\")\n    print(X_train.shape)\n    pnn_batch = PNN(sigma=best_sigma, batch_size=1000)\n    pnn_batch.fit(X_train, y_train)\n    y_pred = pnn_batch.predict(X_test)\n\n    print(\"Classification Report:\")\n    print(classification_report(y_test, y_pred))\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(y_test, y_pred))\n\n    # Optionally, plot the confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Rain\", \"Rain\"], yticklabels=[\"No Rain\", \"Rain\"])\n    plt.title(\"Confusion Matrix\")\n    plt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:47:31.633489Z","iopub.execute_input":"2025-05-03T07:47:31.634087Z","iopub.status.idle":"2025-05-03T07:47:45.026655Z","shell.execute_reply.started":"2025-05-03T07:47:31.634069Z","shell.execute_reply":"2025-05-03T07:47:45.025967Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### SCG","metadata":{}},{"cell_type":"code","source":"param_grid = {'hidden_dim': [10, 20, 50, 100, 150, 17, 25, 34, 50, 68, 85]}\n\nfor name, X_trval, y_trval, X_test, y_test in balanced_models_smote:\n    print(f\"\\n=== Dataset: {name} ===\")\n    # split train+val from test\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_trval, y_trval, test_size=0.2, random_state=42\n    )\n    # one-hot encode y_train\n    y_arr = y_train.to_numpy().reshape(-1, 1)\n    encoder = OneHotEncoder(sparse=False, categories='auto')\n    Y_train_oh = encoder.fit_transform(y_arr.reshape(-1,1))\n    y_arv = y_val.to_numpy().reshape(-1, 1)\n    Y_val_oh   = encoder.transform(y_arv.reshape(-1,1))\n    # final test encoding not needed for training\n    \n    # scale assumed done before, X_trval and X_test are already scaled\n    \n    input_dim  = X_train.shape[1]\n    output_dim = Y_train_oh.shape[1]\n    \n    best_h, best_loss = None, np.inf\n    for h in param_grid['hidden_dim']:\n        # init params\n        param_size  = h*input_dim + h + output_dim*h + output_dim\n        init_p = 0.01 * np.random.randn(param_size)\n        # train SCG\n        lg = loss_grad_factory(X_train, Y_train_oh, input_dim, h, output_dim)\n        opt_p, _ = trainscg(lg, init_p, epochs=8)\n        # predict on val\n        W1, b1, W2, b2 = unpack(opt_p, input_dim, h, output_dim)\n        Z1_val = X_val.dot(W1.T) + b1\n        A1_val = np.tanh(Z1_val)\n        Z2_val = A1_val.dot(W2.T) + b2\n        A2_val = softmax(Z2_val)\n        val_loss = -np.mean(np.sum(Y_val_oh * np.log(A2_val + 1e-12), axis=1))\n        print(f\" hidden_dim={h:<3} → val loss = {val_loss:.6f}\")\n        if val_loss < best_loss:\n            best_loss, best_h = val_loss, h\n    \n    print(f\"⇒ Best hidden_dim = {best_h}, validation loss = {best_loss:.6f}\")\n    \n    # retrain on full train+val\n    X_full = np.vstack([X_train, X_val])\n    y_full = np.hstack([y_train, y_val])\n    Y_full_oh = encoder.fit_transform(y_full.reshape(-1,1))\n    param_size  = best_h*input_dim + best_h + output_dim*best_h + output_dim\n    init_p_full = 0.01 * np.random.randn(param_size)\n    lg_full = loss_grad_factory(X_full, Y_full_oh, input_dim, best_h, output_dim)\n    opt_p_full, loss_hist = trainscg(lg_full, init_p_full, epochs=8)\n    \n    # evaluate on test\n    W1f, b1f, W2f, b2f = unpack(opt_p_full, input_dim, best_h, output_dim)\n    A1_test = np.tanh(X_test.dot(W1f.T) + b1f)\n    probs_test = softmax(A1_test.dot(W2f.T) + b2f)\n    preds_test = np.argmax(probs_test, axis=1)\n    \n    print(\"Test classification report:\")\n    print(classification_report(y_test, preds_test))\n    cm = confusion_matrix(y_test, preds_test)\n    print(\"Test confusion matrix:\\n\", cm)\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n    plt.title(f\"{name} – hidden_dim={best_h}\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:51:53.684836Z","iopub.execute_input":"2025-05-03T07:51:53.685329Z","iopub.status.idle":"2025-05-03T07:57:01.711245Z","shell.execute_reply.started":"2025-05-03T07:51:53.685311Z","shell.execute_reply":"2025-05-03T07:57:01.710641Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SMOTE CNN","metadata":{}},{"cell_type":"code","source":"# # Dùng SMOTE + CNN\n# balanced_models_smotecnn = balance_datasets(models_info, method=\"smotecnn\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T08:28:34.281265Z","iopub.status.idle":"2025-04-25T08:28:34.281616Z","shell.execute_reply.started":"2025-04-25T08:28:34.28144Z","shell.execute_reply":"2025-04-25T08:28:34.281478Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SMOTE ENN","metadata":{}},{"cell_type":"code","source":"# # Dùng SMOTE + ENN\nbalanced_models_smoteenn = balance_datasets(models_info, method=\"smoteenn\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T07:57:13.268038Z","iopub.execute_input":"2025-05-03T07:57:13.268722Z","iopub.status.idle":"2025-05-03T08:09:27.257196Z","shell.execute_reply.started":"2025-05-03T07:57:13.268694Z","shell.execute_reply":"2025-05-03T08:09:27.256451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Fine-tune + train\nsigmas = np.logspace(-2, 1, 50)\nparam_grid = {'sigma': sigmas.tolist()}  # Grid search for sigma values\n\nbest_score = 0\nbest_sigma = 0\n\n# Perform grid search over the values of sigma\nprint(\"Fine tuning...\")\nfor name, train_X, train_Y, X_test, y_test in balanced_models_smoteenn:\n    X_train, X_val, y_train, y_val = train_test_split(\n        train_X, train_Y, test_size=0.2, random_state=42\n    )\n    print(f\"{name}...\")\n    for sigma in param_grid['sigma']:\n        pnn_batch = PNN(sigma=sigma, batch_size=2048)\n        pnn_batch.fit(X_train, y_train)\n        y_pred = pnn_batch.predict(X_val)\n        score = classification_report(y_val, y_pred, zero_division=0, output_dict=True)['macro avg']['f1-score']\n    \n        if score > best_score:\n            best_score = score\n            best_sigma = sigma\n\n    print(f\"Best sigma: {best_sigma} with f1: {best_score}\")\n\n# Final evaluation with the best sigma\nprint(\"Train with best param...\")\nfor name, X_train, y_train, X_test, y_test in balanced_models_smoteenn:\n    print(f\"{name}...\")\n    print(X_train.shape)\n    pnn_batch = PNN(sigma=best_sigma, batch_size=1000)\n    pnn_batch.fit(X_train, y_train)\n    y_pred = pnn_batch.predict(X_test)\n\n    print(\"Classification Report:\")\n    print(classification_report(y_test, y_pred))\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(y_test, y_pred))\n\n    # Optionally, plot the confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Rain\", \"Rain\"], yticklabels=[\"No Rain\", \"Rain\"])\n    plt.title(\"Confusion Matrix\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T08:10:21.964744Z","iopub.execute_input":"2025-05-03T08:10:21.965011Z","iopub.status.idle":"2025-05-03T08:10:33.278691Z","shell.execute_reply.started":"2025-05-03T08:10:21.964993Z","shell.execute_reply":"2025-05-03T08:10:33.27805Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"param_grid = {'hidden_dim': [10, 20, 50, 100, 150, 17, 25, 34, 50, 68, 85]}\n\nfor name, X_trval, y_trval, X_test, y_test in balanced_models_smoteenn:\n    print(f\"\\n=== Dataset: {name} ===\")\n    # split train+val from test\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_trval, y_trval, test_size=0.2, random_state=42\n    )\n    # one-hot encode y_train\n    y_arr = y_train.to_numpy().reshape(-1, 1)\n    encoder = OneHotEncoder(sparse=False, categories='auto')\n    Y_train_oh = encoder.fit_transform(y_arr.reshape(-1,1))\n    y_arv = y_val.to_numpy().reshape(-1, 1)\n    Y_val_oh   = encoder.transform(y_arv.reshape(-1,1))\n    # final test encoding not needed for training\n    \n    # scale assumed done before, X_trval and X_test are already scaled\n    \n    input_dim  = X_train.shape[1]\n    output_dim = Y_train_oh.shape[1]\n    \n    best_h, best_loss = None, np.inf\n    for h in param_grid['hidden_dim']:\n        # init params\n        param_size  = h*input_dim + h + output_dim*h + output_dim\n        init_p = 0.01 * np.random.randn(param_size)\n        # train SCG\n        lg = loss_grad_factory(X_train, Y_train_oh, input_dim, h, output_dim)\n        opt_p, _ = trainscg(lg, init_p, epochs=8)\n        # predict on val\n        W1, b1, W2, b2 = unpack(opt_p, input_dim, h, output_dim)\n        Z1_val = X_val.dot(W1.T) + b1\n        A1_val = np.tanh(Z1_val)\n        Z2_val = A1_val.dot(W2.T) + b2\n        A2_val = softmax(Z2_val)\n        val_loss = -np.mean(np.sum(Y_val_oh * np.log(A2_val + 1e-12), axis=1))\n        print(f\" hidden_dim={h:<3} → val loss = {val_loss:.6f}\")\n        if val_loss < best_loss:\n            best_loss, best_h = val_loss, h\n    \n    print(f\"⇒ Best hidden_dim = {best_h}, validation loss = {best_loss:.6f}\")\n    \n    # retrain on full train+val\n    X_full = np.vstack([X_train, X_val])\n    y_full = np.hstack([y_train, y_val])\n    Y_full_oh = encoder.fit_transform(y_full.reshape(-1,1))\n    param_size  = best_h*input_dim + best_h + output_dim*best_h + output_dim\n    init_p_full = 0.01 * np.random.randn(param_size)\n    lg_full = loss_grad_factory(X_full, Y_full_oh, input_dim, best_h, output_dim)\n    opt_p_full, loss_hist = trainscg(lg_full, init_p_full, epochs=8)\n    \n    # evaluate on test\n    W1f, b1f, W2f, b2f = unpack(opt_p_full, input_dim, best_h, output_dim)\n    A1_test = np.tanh(X_test.dot(W1f.T) + b1f)\n    probs_test = softmax(A1_test.dot(W2f.T) + b2f)\n    preds_test = np.argmax(probs_test, axis=1)\n    \n    print(\"Test classification report:\")\n    print(classification_report(y_test, preds_test))\n    cm = confusion_matrix(y_test, preds_test)\n    print(\"Test confusion matrix:\\n\", cm)\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n    plt.title(f\"{name} – hidden_dim={best_h}\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T08:14:38.637252Z","iopub.execute_input":"2025-05-03T08:14:38.63754Z","iopub.status.idle":"2025-05-03T08:19:15.482719Z","shell.execute_reply.started":"2025-05-03T08:14:38.63752Z","shell.execute_reply":"2025-05-03T08:19:15.481935Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}